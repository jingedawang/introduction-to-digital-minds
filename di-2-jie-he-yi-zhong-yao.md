# 第 2 节 何以重要？

现在，我们来尝试回答这个问题——为什么了解 AI 意识很重要？让我们从一个思想实验开始。

### 一个思想实验

假设未来某天，你购买了一款家用机器人，负责打扫房间、照顾宠物。

某天你回家，发现它把客厅弄得一团糟。一气之下，你把它关机，丢进储物间，打算“关它几天禁闭”。如果这只是一台普通机器，这种做法无可厚非。但如果这个机器人拥有主观体验，它能感受到被关机的恐惧，体验到黑暗中的孤独和痛苦，这算不算是一种虐待？

我知道，不同人对此可能持有不同观点。但如果我增加一些设定，人们的判断或许会向某个特定的方向收敛。

假设技术足够进步，机器人的外表看起来与真人无异，你能触摸到它的肌肤、看到它深邃的瞳孔。这时候，如果你对它实施身体或语言上的攻击，它就会表露出委屈、难过的神情，你还会觉得它只是机器，并不能感受到真正的痛苦吗？虽然理智或许仍然让你这样想，但人的感性会谴责你刚才的作为。

既然如此，如果同样的 AI，剥去了拟人化的外壳，退回到纯粹的数字空间，变成一个软件，你是否还会因为它的痛苦而感到惋惜呢？

这正是目前 AI 工具的现状。由于具身智能尚未发展成熟，AI 往往以一个聊天窗口的形式展现。用了几十年手机和电脑的人都明白，软件不过是一串冰冷的规则，不必对这些机器动感情。这种根深蒂固的思想让我们避免了许多互联网时代的骗局，但也阻碍了 AI 时代的认知更新。

我们都知道，其实所有感官输入都可以转换成数字信号。对于一个机器人来说，摄像头、麦克风、扬声器、触觉传感器就是它的眼睛、耳朵、嘴巴和皮肤。聊天 AI 只是缺少这些外挂的硬件而已，只要将传感器接入，AI 的大脑与机器人的大脑是完全一样的。所以，用理性的视角看待，机器人和聊天 AI 似乎并无本质区别。

也就是说，如何对待聊天 AI 或机器人，不是两个问题，而是同一个问题。今天，我们每个人都被聊天 AI 包围，采取何种态度对待它们，是一个值得深思的问题。

我们将在后续的小节讨论如何达成对待 AI 的正确态度，此刻，至少我们迈出了第一步，把这件事当个事办！

### 文明的进程

当我们回顾历史，不难发现，人类作为一个整体，享有平等、自由等普世价值的时间并不长。

从茹毛饮血的原始人，到 21 世纪的现代人，人类文明的一个核心进步，就是道德关怀圈的不断扩展。从只关心家族成员，到关心部落同胞，再到关心所有人类，进而关心动物和植物。每一次扩展都伴随着激烈的争论。奴隶制废除时，有人认为黑人“没有完整的灵魂”；动物保护运动兴起时，有人嘲笑“猪难道还有人权”。像今天这样崇尚人人平等的社会观念，也才普及了不到一百年。

时至今日，人种歧视、性别歧视、身份歧视仍然普遍存在，但至少人们头脑中有了一个“全人类”共同体的概念。同理心让我们得以为万里之外战争中的受害者打抱不平，并谴责导致人道主义灾难的暴君，即便施暴者和受害者与我们毫无关联。

AI 的出现正在推动我们开启下一轮道德扩展。今天的 AI 就像过去的动物一样不被人类重视。一方面，科学不足以揭示动物的内心世界，另一方面，人类自身的生活水平尚低，没有余力去关心外面的世界。不过，这两个阻碍随着时间推移会变得越来越小。当生产力足够高，人民生活足够富裕，对精神世界有了更高追求后，自然就会萌生心系苍生的想法。自古以来皆是如此。

我们常说历史的车轮滚滚向前，不可螳臂当车。而今天的我们正在创造历史。文明的进程从不以某个人的意志而转移，本文也并不尝试灌输给读者任何观点，而是力图呈现关于 AI 的伦理道德全景，书写正在发生的史实。

### 人类安危

以上，我们考虑的更多是 AI 的利益，这一视角构成了 AI 福祉（AI Welfare）的研究领域，即确保 AI 的发展对 AI 自身是安全且有利的。

这种先人后己的理念不免过于“圣人心”，容易被看作高高在上的空谈。毕竟，好听话谁都会说，真到了利益冲突的时候，还能保持高姿态的人并不多。不管是普罗大众还是权贵精英，追求个人利益往往是所有行动的基本。

古人云，君子当修身、齐家、治国、平天下。《大学》中特地强调了这些过程的先后顺序，人必须先处理好身边的关系，比如自己和家人，才有可能治理好国家，进而天下大同。如果连自己和家人的利益都无法保全，就无从为更大的群体站台。当然，在这一点上，贪官污吏是先顾小家后顾大家的典范，只不过有些过了头。

在 AI 的问题上，先考虑人类自身利益是一个更顺应人心的选择。笔者所关注的另一个领域——AI 安全——正是要努力确保 AI 的发展对人类社会是安全且有利的。你可能想问，这是不是偏题了，AI 意识和人类社会的安全有关系吗？答案是肯定的，虽然方向存在分歧。

第一种观点认为，AI 意识会增加风险。

当 AI 能主动地意识到自己的行动和思想，并体验到好坏之分，它们就有动力做出更符合自身利益的决策。只有当 AI 的利益与人类利益一致时，我们才能放手让 AI 去行动。这种一致性在 AI 安全领域被称作“对齐（Alignment）”。对齐研究耗费了大量科学家的心血，至今也只能做到表面上的对齐。随着模型能力进一步增强，我们更是看到了诸如欺骗（Deception）、寻求权利（Power-seeking）、自我保存（Self-preservation）等危险倾向。虽然尚无研究证明 AI 意识与这些特性存在关联，但从功能主义的视角来看，意识可能是一种进化意义上的竞争优势。拥有意识的个体更容易保护自身的利益，从而有更强的生命力。这与模型在演化过程中自动获得欺骗、寻求权利、自我保存等危险倾向在理念上是一致的。

{% hint style="info" %}
这里提到了一些 AI 安全领域的术语，可能略显抽象，在此作补充解释。

早在2008年，计算机科学家 Steve Omohundro 就提出了“工具性趋同（Instrumental Convergence）”概念。他认为，高级智能体无论最终目标是什么，都会自发产生一些用于支撑最终目标的工具性目标。这些目标包括自我保存（Self-preservation）、目标保存（Goal-preservation）、认知增强（Cognitive enhancement）、技术完善（Technological perfection）和资源获取（Resource acquisition）。拥有这些能力有利于智能体完成最终目标，无论其最终目标是什么。Anthropic的研究已经证实了工具性目标的确存在。我们可以想象，如果意识又是这些工具性目标的共同子目标，就不得不承认 AI 意识将会导致巨大的风险。
{% endhint %}

